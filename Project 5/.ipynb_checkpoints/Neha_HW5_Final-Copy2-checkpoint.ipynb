{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###### importing important libraries including numpy, pandas and pyTorch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import math\n",
    "\n",
    "# for evaluating the model\n",
    "from sklearn.metrics import accuracy_score\n",
    "from tqdm import tqdm\n",
    "\n",
    "# PyTorch libraries and modules\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.autograd import Variable\n",
    "import torch.nn.functional as F\n",
    "from torch.nn import Linear, ReLU, CrossEntropyLoss, Sequential, Conv2d, MaxPool2d, Module, Softmax, BatchNorm2d, Dropout\n",
    "from torch.optim import Adam, SGD\n",
    "from torch.nn.utils.rnn import pack_padded_sequence, pad_packed_sequence\n",
    "import itertools"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###### Load the data, convert language labels into int form"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "df_train= pd.read_csv('/Users/nehakardam/Documents/UWclasses /EE511- Intro to Statistics/Neha_Assignment/HW5/train2.csv', names=['language', 'document'])\n",
    "df_val= pd.read_csv('/Users/nehakardam/Documents/UWclasses /EE511- Intro to Statistics/Neha_Assignment/HW5/val2.csv', names=['language', 'document'])\n",
    "df_test= pd.read_csv('/Users/nehakardam/Documents/UWclasses /EE511- Intro to Statistics/Neha_Assignment/HW5/test2.csv', names=['language', 'document'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'es': 0,\n",
       " 'en': 1,\n",
       " 'pt': 2,\n",
       " 'fr': 3,\n",
       " 'ca': 4,\n",
       " 'de': 5,\n",
       " 'eu': 6,\n",
       " 'it': 7,\n",
       " 'gl': 8}"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "language_map = {}\n",
    "i = 0\n",
    "for lan in df_train.language.unique():\n",
    "    language_map[lan] = i\n",
    "    i = i+1\n",
    "df_train.language.replace(language_map, inplace=True)\n",
    "df_val.language.replace(language_map, inplace=True)\n",
    "df_test.language.replace(language_map, inplace=True)\n",
    "language_map"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Creating a vocabulury and computing the perplexity for this distribution using the validation data.\n",
    "\n",
    "cross_entropy: 5.0599491674909345 \n",
    "\n",
    "perplexity: 33.35772898629834"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cross_entropy: 5.0599491674909345 \n",
      "perplexity: 33.35772898629834\n"
     ]
    }
   ],
   "source": [
    "def text_to_vocab(text_arr):\n",
    "    v = {}\n",
    "    all_freq = {} \n",
    "    for text in text_arr:\n",
    "        for i in text: \n",
    "            if i in all_freq: \n",
    "                all_freq[i] += 1\n",
    "            else: \n",
    "                all_freq[i] = 1\n",
    "    v[spl_char] = 0\n",
    "    for i in all_freq:\n",
    "        if(all_freq[i] >= 10):\n",
    "            v[i] = all_freq[i]\n",
    "        else:\n",
    "            v[spl_char] = v[spl_char] + all_freq[i]\n",
    "    return v\n",
    "\n",
    "spl_char = 'out-of-vocabulary'\n",
    "vocab = {}\n",
    "\n",
    "train_tweets = []\n",
    "for i in range(df_train.shape[0]):\n",
    "    text = df_train.document[i]\n",
    "    train_tweets.append(text)\n",
    "\n",
    "vocab = text_to_vocab(train_tweets)\n",
    "\n",
    "relative_freq = {}\n",
    "s = sum(vocab.values())\n",
    "for x in vocab:\n",
    "    relative_freq[x] = vocab[x]/s\n",
    "    \n",
    "val_tweets = []\n",
    "for i in range(df_val.shape[0]):\n",
    "    text = df_val.document[i]\n",
    "    val_tweets.append(text)\n",
    "    \n",
    "logpx = []\n",
    "for t in val_tweets:\n",
    "    for x in t:\n",
    "        if x in relative_freq:\n",
    "            logpx.append(math.log2(relative_freq[x]))\n",
    "        else:\n",
    "            logpx.append(math.log2(relative_freq[spl_char]))\n",
    "\n",
    "cross_entropy = (-1)*sum(logpx)/len(logpx)\n",
    "perplexity = 2**cross_entropy\n",
    "\n",
    "print('cross_entropy:', cross_entropy, '\\nperplexity:', perplexity)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###### Converting text data into tensor sequenes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def index_of(tok):\n",
    "    vocab_list = list(vocab.keys())\n",
    "    if tok in vocab_list:\n",
    "        return vocab_list.index(tok)\n",
    "    else:\n",
    "        return 0\n",
    "\n",
    "train_tweets = df_train.document.str.slice(0, 280)\n",
    "val_tweets = df_val.document.str.slice(0, 280)\n",
    "train_vectorized_seqs = [[index_of(tok) for tok in seq]for seq in train_tweets]\n",
    "val_vectorized_seqs = [[index_of(tok) for tok in seq]for seq in val_tweets]\n",
    "\n",
    "train_seq_lengths = torch.LongTensor(list(map(len, train_vectorized_seqs)))\n",
    "val_seq_lengths = torch.LongTensor(list(map(len, val_vectorized_seqs)))\n",
    "\n",
    "train_seq_tensor = Variable(torch.zeros((len(train_vectorized_seqs), 280))).long()\n",
    "for idx, (seq, seqlen) in enumerate(zip(train_vectorized_seqs, train_seq_lengths)):\n",
    "    train_seq_tensor[idx, :seqlen] = torch.LongTensor(seq)\n",
    "    \n",
    "val_seq_tensor = Variable(torch.zeros((len(val_vectorized_seqs), 280))).long()\n",
    "for idx, (seq, seqlen) in enumerate(zip(val_vectorized_seqs, val_seq_lengths)):\n",
    "    val_seq_tensor[idx, :seqlen] = torch.LongTensor(seq)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###### Perform embedding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "embedding = nn.Embedding(len(vocab), 16)\n",
    "\n",
    "train_x = embedding(train_seq_tensor)\n",
    "val_x = embedding(val_seq_tensor)\n",
    "\n",
    "train_y = torch.tensor(df_train.language.values)\n",
    "val_y = torch.tensor(df_val.language.values)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([76875, 280, 16])"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_x.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "# prepping the training and validation set for CNN model\n",
    "x_train, x_val = train_x.reshape(76875, 1, 280, 16), val_x.reshape(11128, 1, 280, 16)\n",
    "y_train, y_val = Variable(train_y), Variable(val_y)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###### Define CNN model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Net(\n",
      "  (layer1): Sequential(\n",
      "    (0): Conv2d(1, 50, kernel_size=(5, 5), stride=(1, 1), padding=(2, 2))\n",
      "    (1): ReLU()\n",
      "    (2): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
      "  )\n",
      "  (layer2): Sequential(\n",
      "    (0): Conv2d(50, 100, kernel_size=(5, 5), stride=(1, 1), padding=(2, 2))\n",
      "    (1): ReLU()\n",
      "    (2): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
      "  )\n",
      "  (drop_out): Dropout(p=0.5, inplace=False)\n",
      "  (fc1): Linear(in_features=28000, out_features=1000, bias=True)\n",
      "  (fc2): Linear(in_features=1000, out_features=9, bias=True)\n",
      ")\n"
     ]
    }
   ],
   "source": [
    "class Net(Module):   \n",
    "    def __init__(self):\n",
    "        super(Net, self).__init__()\n",
    "        self.layer1 = nn.Sequential(\n",
    "            nn.Conv2d(1, 50, kernel_size=5, stride=1, padding=2),\n",
    "            nn.ReLU(),\n",
    "            nn.MaxPool2d(kernel_size=2, stride=2))\n",
    "        self.layer2 = nn.Sequential(\n",
    "            nn.Conv2d(50, 100, kernel_size=5, stride=1, padding=2),\n",
    "            nn.ReLU(),\n",
    "            nn.MaxPool2d(kernel_size=2, stride=2))\n",
    "        self.drop_out = nn.Dropout()\n",
    "        self.fc1 = nn.Linear(100 * 280, 1000)\n",
    "        self.fc2 = nn.Linear(1000, 9)\n",
    "\n",
    "    def forward(self, x):\n",
    "        out = self.layer1(x)\n",
    "        out = self.layer2(out)\n",
    "        out = out.reshape(out.size(0), -1)\n",
    "        out = self.drop_out(out)\n",
    "        out = self.fc1(out)\n",
    "        out = self.fc2(out)\n",
    "        return out\n",
    "\n",
    "# defining the model\n",
    "model = Net()\n",
    "# defining the optimizer\n",
    "optimizer = Adam(model.parameters(), lr=0.07)\n",
    "# defining the loss function\n",
    "criterion = CrossEntropyLoss()\n",
    "    \n",
    "print(model)\n",
    "\n",
    "\n",
    "def train(x_train, y_train):\n",
    "    model.train()\n",
    "    tr_loss = 0\n",
    "\n",
    "    # clearing the Gradients of the model parameters\n",
    "    optimizer.zero_grad()\n",
    "    \n",
    "    # prediction for training set\n",
    "    output = model(x_train)\n",
    "\n",
    "    # computing the training and validation loss\n",
    "    loss = criterion(output, y_train)\n",
    "\n",
    "    # computing the updated weights of all the model parameters\n",
    "    loss.backward(retain_graph=True)\n",
    "    optimizer.step()\n",
    "    tr_loss = loss.item()\n",
    "\n",
    "    # Track the accuracy\n",
    "    total = y_train.shape[0]\n",
    "    _, predicted = torch.max(output.data, 1)\n",
    "    correct = (predicted == y_train).sum().item()\n",
    "\n",
    "    print('Loss: {:.4f}, Accuracy: {:.2f}%'.format(tr_loss, (correct / total) * 100))\n",
    "    \n",
    "    return loss"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###### Train the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loss: 2.1870, Accuracy: 14.00%\n",
      "Loss: 37552.3438, Accuracy: 28.00%\n",
      "Loss: 329501.4062, Accuracy: 1.00%\n",
      "Loss: 27108.4883, Accuracy: 32.00%\n",
      "Loss: 2259.7744, Accuracy: 21.00%\n",
      "Loss: 4.6153, Accuracy: 24.00%\n",
      "Loss: 4.4603, Accuracy: 34.00%\n",
      "Loss: 5.1571, Accuracy: 32.00%\n",
      "Loss: 2.3933, Accuracy: 22.00%\n",
      "Loss: 4.6724, Accuracy: 16.00%\n",
      "Loss: 4.0023, Accuracy: 32.00%\n",
      "Loss: 3.2943, Accuracy: 38.00%\n",
      "Loss: 2.5801, Accuracy: 30.00%\n",
      "Loss: 3.6233, Accuracy: 36.00%\n",
      "Loss: 4.2683, Accuracy: 28.00%\n",
      "Loss: 2.1008, Accuracy: 40.00%\n",
      "Loss: 4.4762, Accuracy: 33.00%\n",
      "Loss: 2.5481, Accuracy: 36.00%\n",
      "Loss: 4.0940, Accuracy: 27.00%\n",
      "Loss: 4.9378, Accuracy: 34.00%\n",
      "Loss: 5.2338, Accuracy: 32.00%\n",
      "Loss: 3.8575, Accuracy: 27.00%\n",
      "Loss: 3.7253, Accuracy: 26.00%\n",
      "Loss: 2.1043, Accuracy: 41.00%\n",
      "Loss: 3.1715, Accuracy: 28.00%\n",
      "Loss: 4.5826, Accuracy: 32.00%\n",
      "Loss: 2.7347, Accuracy: 33.00%\n",
      "Loss: 4.3988, Accuracy: 35.00%\n",
      "Loss: 3.1269, Accuracy: 40.00%\n",
      "Loss: 2.7172, Accuracy: 31.00%\n",
      "Loss: 3.8647, Accuracy: 36.00%\n",
      "Loss: 5.0039, Accuracy: 32.00%\n",
      "Loss: 2.5572, Accuracy: 29.00%\n",
      "Loss: 4.2993, Accuracy: 26.00%\n",
      "Loss: 3.0184, Accuracy: 24.00%\n",
      "Loss: 3.6552, Accuracy: 33.00%\n",
      "Loss: 5.8829, Accuracy: 30.00%\n",
      "Loss: 5.3132, Accuracy: 29.00%\n",
      "Loss: 4.2401, Accuracy: 28.00%\n",
      "Loss: 1.6782, Accuracy: 27.00%\n",
      "Loss: 4.7814, Accuracy: 30.00%\n",
      "Loss: 1.7535, Accuracy: 32.00%\n",
      "Loss: 3.9693, Accuracy: 20.00%\n",
      "Loss: 3.9640, Accuracy: 30.00%\n",
      "Loss: 3.4793, Accuracy: 25.00%\n",
      "Loss: 1.7520, Accuracy: 15.00%\n",
      "Loss: 6.0962, Accuracy: 28.00%\n",
      "Loss: 3.6009, Accuracy: 40.00%\n",
      "Loss: 2.5155, Accuracy: 38.00%\n",
      "Loss: 3.9938, Accuracy: 36.00%\n",
      "Loss: 6.1994, Accuracy: 24.00%\n",
      "Loss: 3.0773, Accuracy: 25.00%\n",
      "Loss: 1.6280, Accuracy: 27.00%\n",
      "Loss: 3.9984, Accuracy: 32.00%\n",
      "Loss: 1.5892, Accuracy: 28.00%\n",
      "Loss: 3.2662, Accuracy: 26.00%\n",
      "Loss: 3.3985, Accuracy: 29.00%\n",
      "Loss: 3.1358, Accuracy: 32.00%\n",
      "Loss: 1.7024, Accuracy: 37.00%\n",
      "Loss: 3.7328, Accuracy: 31.00%\n",
      "Loss: 1.6914, Accuracy: 33.00%\n",
      "Loss: 2.4476, Accuracy: 30.00%\n",
      "Loss: 1.9215, Accuracy: 18.00%\n",
      "Loss: 3.2488, Accuracy: 30.00%\n",
      "Loss: 1.5862, Accuracy: 28.00%\n",
      "Loss: 2.3095, Accuracy: 30.00%\n",
      "Loss: 1.6677, Accuracy: 28.00%\n",
      "Loss: 3.1108, Accuracy: 30.00%\n",
      "Loss: 1.9409, Accuracy: 25.00%\n",
      "Loss: 2.2159, Accuracy: 21.00%\n",
      "Loss: 44.8842, Accuracy: 39.00%\n",
      "Loss: 2.8997, Accuracy: 34.00%\n",
      "Loss: 1.6872, Accuracy: 29.00%\n",
      "Loss: 2.5301, Accuracy: 35.00%\n",
      "Loss: 1.7543, Accuracy: 27.00%\n",
      "Loss: 4.3166, Accuracy: 26.00%\n",
      "Loss: 1.5801, Accuracy: 34.00%\n",
      "Loss: 2.7519, Accuracy: 30.00%\n",
      "Loss: 3.3754, Accuracy: 34.00%\n",
      "Loss: 2.1219, Accuracy: 36.00%\n",
      "Loss: 4.0441, Accuracy: 34.00%\n",
      "Loss: 2.9265, Accuracy: 32.00%\n",
      "Loss: 3.0704, Accuracy: 31.00%\n",
      "Loss: 5.0671, Accuracy: 24.00%\n",
      "Loss: 5.0418, Accuracy: 28.00%\n",
      "Loss: 3.1442, Accuracy: 36.00%\n",
      "Loss: 2.8680, Accuracy: 34.00%\n",
      "Loss: 3.2317, Accuracy: 30.00%\n",
      "Loss: 2.4377, Accuracy: 31.00%\n",
      "Loss: 3.8610, Accuracy: 29.00%\n",
      "Loss: 3.9678, Accuracy: 34.00%\n",
      "Loss: 1.8000, Accuracy: 32.00%\n",
      "Loss: 4.2593, Accuracy: 35.00%\n",
      "Loss: 3.3752, Accuracy: 34.00%\n",
      "Loss: 2.8723, Accuracy: 38.00%\n",
      "Loss: 3.9654, Accuracy: 29.00%\n",
      "Loss: 4.6142, Accuracy: 31.00%\n",
      "Loss: 2.5171, Accuracy: 32.00%\n",
      "Loss: 4.4620, Accuracy: 25.00%\n",
      "Loss: 2.9372, Accuracy: 31.00%\n",
      "Loss: 3.4859, Accuracy: 27.00%\n",
      "Loss: 5.6783, Accuracy: 22.00%\n",
      "Loss: 3.5445, Accuracy: 28.00%\n",
      "Loss: 1.7397, Accuracy: 29.00%\n",
      "Loss: 5.0145, Accuracy: 33.00%\n",
      "Loss: 4.3146, Accuracy: 26.00%\n",
      "Loss: 3.0825, Accuracy: 33.00%\n",
      "Loss: 4.3391, Accuracy: 32.00%\n",
      "Loss: 4.6759, Accuracy: 29.00%\n",
      "Loss: 5.0640, Accuracy: 26.00%\n",
      "Loss: 2.1255, Accuracy: 32.00%\n",
      "Loss: 5.3855, Accuracy: 29.00%\n",
      "Loss: 4.8630, Accuracy: 32.00%\n",
      "Loss: 2.0744, Accuracy: 35.00%\n",
      "Loss: 2.9506, Accuracy: 42.00%\n",
      "Loss: 3.5660, Accuracy: 29.00%\n",
      "Loss: 1.7656, Accuracy: 33.00%\n",
      "Loss: 4.7681, Accuracy: 28.00%\n",
      "Loss: 2.8677, Accuracy: 30.00%\n",
      "Loss: 3.1474, Accuracy: 30.00%\n",
      "Loss: 4.3148, Accuracy: 32.00%\n",
      "Loss: 5.6164, Accuracy: 32.00%\n",
      "Loss: 4.9540, Accuracy: 24.00%\n",
      "Loss: 2.1819, Accuracy: 32.00%\n",
      "Loss: 5.3214, Accuracy: 35.00%\n",
      "Loss: 6.7855, Accuracy: 28.00%\n",
      "Loss: 1.6391, Accuracy: 31.00%\n",
      "Loss: 2.9696, Accuracy: 36.00%\n",
      "Loss: 6.1641, Accuracy: 27.00%\n",
      "Loss: 5.6205, Accuracy: 33.00%\n",
      "Loss: 4.0679, Accuracy: 31.00%\n",
      "Loss: 2.0448, Accuracy: 25.00%\n",
      "Loss: 6.1534, Accuracy: 34.00%\n",
      "Loss: 7.5124, Accuracy: 28.00%\n",
      "Loss: 2.1030, Accuracy: 30.00%\n",
      "Loss: 5.4100, Accuracy: 25.00%\n",
      "Loss: 7.8316, Accuracy: 32.00%\n",
      "Loss: 6.3305, Accuracy: 40.00%\n",
      "Loss: 5.8750, Accuracy: 34.00%\n",
      "Loss: 4.6910, Accuracy: 28.00%\n",
      "Loss: 2.0147, Accuracy: 38.00%\n",
      "Loss: 4.8662, Accuracy: 36.00%\n",
      "Loss: 3.1935, Accuracy: 27.00%\n",
      "Loss: 4.3354, Accuracy: 23.00%\n",
      "Loss: 5.0280, Accuracy: 27.00%\n",
      "Loss: 5.1852, Accuracy: 32.00%\n",
      "Loss: 4.3131, Accuracy: 24.00%\n",
      "Loss: 2.7274, Accuracy: 30.00%\n",
      "Loss: 3.8730, Accuracy: 48.00%\n",
      "Loss: 7.0718, Accuracy: 31.00%\n",
      "Loss: 3.2020, Accuracy: 31.00%\n",
      "Loss: 4.7582, Accuracy: 26.00%\n",
      "Loss: 6.6958, Accuracy: 32.00%\n",
      "Loss: 6.2709, Accuracy: 35.00%\n",
      "Loss: 5.1244, Accuracy: 26.00%\n",
      "Loss: 3.1218, Accuracy: 26.00%\n",
      "Loss: 3.5833, Accuracy: 31.00%\n",
      "Loss: 4.2288, Accuracy: 30.00%\n",
      "Loss: 1.8373, Accuracy: 36.00%\n",
      "Loss: 2.8125, Accuracy: 33.00%\n",
      "Loss: 3.4692, Accuracy: 29.00%\n",
      "Loss: 1.9479, Accuracy: 31.00%\n",
      "Loss: 3.2148, Accuracy: 34.00%\n",
      "Loss: 2.4733, Accuracy: 32.00%\n",
      "Loss: 2.8364, Accuracy: 21.00%\n",
      "Loss: 4.5192, Accuracy: 33.00%\n",
      "Loss: 2.9151, Accuracy: 32.00%\n",
      "Loss: 1.6653, Accuracy: 34.00%\n",
      "Loss: 3.8268, Accuracy: 30.00%\n",
      "Loss: 2.0717, Accuracy: 29.00%\n",
      "Loss: 3.5207, Accuracy: 29.00%\n",
      "Loss: 4.3636, Accuracy: 29.00%\n",
      "Loss: 3.9259, Accuracy: 25.00%\n",
      "Loss: 1.7538, Accuracy: 36.00%\n",
      "Loss: 5.0315, Accuracy: 28.00%\n",
      "Loss: 2.8868, Accuracy: 37.00%\n",
      "Loss: 3.1589, Accuracy: 26.00%\n",
      "Loss: 5.1838, Accuracy: 21.00%\n",
      "Loss: 3.5047, Accuracy: 27.00%\n",
      "Loss: 1.4970, Accuracy: 39.00%\n",
      "Loss: 4.5545, Accuracy: 35.00%\n",
      "Loss: 3.1429, Accuracy: 33.00%\n",
      "Loss: 3.0565, Accuracy: 37.00%\n",
      "Loss: 5.1086, Accuracy: 33.00%\n",
      "Loss: 4.8017, Accuracy: 27.00%\n",
      "Loss: 3.1422, Accuracy: 29.00%\n",
      "Loss: 3.1384, Accuracy: 40.00%\n",
      "Loss: 4.8332, Accuracy: 25.00%\n",
      "Loss: 1.7342, Accuracy: 38.00%\n",
      "Loss: 3.8077, Accuracy: 25.00%\n",
      "Loss: 2.6196, Accuracy: 26.00%\n",
      "Loss: 1.7511, Accuracy: 31.00%\n",
      "Loss: 2.5015, Accuracy: 31.00%\n",
      "Loss: 1.9630, Accuracy: 32.00%\n",
      "Loss: 2.0960, Accuracy: 36.00%\n",
      "Loss: 1.8137, Accuracy: 22.00%\n",
      "Loss: 2.8328, Accuracy: 31.00%\n",
      "Loss: 1.5647, Accuracy: 32.00%\n",
      "Loss: 2.5483, Accuracy: 26.00%\n",
      "Loss: 2.3216, Accuracy: 33.00%\n",
      "Loss: 1.8736, Accuracy: 34.00%\n",
      "Loss: 2.1412, Accuracy: 27.00%\n",
      "Loss: 2.5738, Accuracy: 32.00%\n",
      "Loss: 2.2746, Accuracy: 33.00%\n",
      "Loss: 1.4395, Accuracy: 34.00%\n",
      "Loss: 2.5799, Accuracy: 32.00%\n",
      "Loss: 1.6857, Accuracy: 35.00%\n",
      "Loss: 2.2075, Accuracy: 32.00%\n",
      "Loss: 1.4549, Accuracy: 29.00%\n",
      "Loss: 2.7967, Accuracy: 30.00%\n",
      "Loss: 1.8428, Accuracy: 30.00%\n",
      "Loss: 2.1780, Accuracy: 26.00%\n",
      "Loss: 1.7414, Accuracy: 26.00%\n",
      "Loss: 2.4213, Accuracy: 32.00%\n",
      "Loss: 1.8497, Accuracy: 32.00%\n",
      "Loss: 1.8190, Accuracy: 33.00%\n",
      "Loss: 2.3073, Accuracy: 30.00%\n",
      "Loss: 1.6788, Accuracy: 31.00%\n",
      "Loss: 1.6647, Accuracy: 37.00%\n",
      "Loss: 1.5877, Accuracy: 32.00%\n",
      "Loss: 1.5350, Accuracy: 37.00%\n",
      "Loss: 1.5231, Accuracy: 35.00%\n",
      "Loss: 1.6718, Accuracy: 26.00%\n",
      "Loss: 1.5927, Accuracy: 26.00%\n",
      "Loss: 1.5547, Accuracy: 29.00%\n",
      "Loss: 1.6397, Accuracy: 36.00%\n",
      "Loss: 1.6431, Accuracy: 31.00%\n",
      "Loss: 1.4339, Accuracy: 33.00%\n",
      "Loss: 1.5424, Accuracy: 34.00%\n",
      "Loss: 1.5647, Accuracy: 27.00%\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-9-72c08f201233>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      7\u001b[0m \u001b[0;31m# training the model\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      8\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mx_t\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my_t\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mzip\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbatch\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx_train\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbatch\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0my_train\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 9\u001b[0;31m     \u001b[0mloss\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtrain\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx_t\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my_t\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     10\u001b[0m     \u001b[0mloss\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdetach\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-8-fd1f8e2f310a>\u001b[0m in \u001b[0;36mtrain\u001b[0;34m(x_train, y_train)\u001b[0m\n\u001b[1;32m     47\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     48\u001b[0m     \u001b[0;31m# computing the updated weights of all the model parameters\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 49\u001b[0;31m     \u001b[0mloss\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbackward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mretain_graph\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     50\u001b[0m     \u001b[0moptimizer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstep\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     51\u001b[0m     \u001b[0mtr_loss\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mloss\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mitem\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/anaconda3/lib/python3.8/site-packages/torch/tensor.py\u001b[0m in \u001b[0;36mbackward\u001b[0;34m(self, gradient, retain_graph, create_graph)\u001b[0m\n\u001b[1;32m    219\u001b[0m                 \u001b[0mretain_graph\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mretain_graph\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    220\u001b[0m                 create_graph=create_graph)\n\u001b[0;32m--> 221\u001b[0;31m         \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mautograd\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbackward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mgradient\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mretain_graph\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcreate_graph\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    222\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    223\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mregister_hook\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mhook\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/anaconda3/lib/python3.8/site-packages/torch/autograd/__init__.py\u001b[0m in \u001b[0;36mbackward\u001b[0;34m(tensors, grad_tensors, retain_graph, create_graph, grad_variables)\u001b[0m\n\u001b[1;32m    128\u001b[0m         \u001b[0mretain_graph\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcreate_graph\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    129\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 130\u001b[0;31m     Variable._execution_engine.run_backward(\n\u001b[0m\u001b[1;32m    131\u001b[0m         \u001b[0mtensors\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mgrad_tensors_\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mretain_graph\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcreate_graph\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    132\u001b[0m         allow_unreachable=True)  # allow_unreachable flag\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "def batch(iterable, n=100):\n",
    "    l = len(iterable)\n",
    "    for ndx in range(0, l, n):\n",
    "        yield iterable[ndx:min(ndx + n, l)]\n",
    "\n",
    "i = 0\n",
    "# training the model    \n",
    "for x_t, y_t in zip(batch(x_train), batch(y_train)):\n",
    "    loss = train(x_t, y_t)\n",
    "    loss.detach()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
