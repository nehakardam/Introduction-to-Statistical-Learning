{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Net(\n",
      "  (layer1): Sequential(\n",
      "    (0): Conv2d(1, 50, kernel_size=(5, 5), stride=(1, 1), padding=(2, 2))\n",
      "    (1): ReLU()\n",
      "    (2): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
      "  )\n",
      "  (layer2): Sequential(\n",
      "    (0): Conv2d(50, 100, kernel_size=(5, 5), stride=(1, 1), padding=(2, 2))\n",
      "    (1): ReLU()\n",
      "    (2): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
      "  )\n",
      "  (drop_out): Dropout(p=0.5, inplace=False)\n",
      "  (fc1): Linear(in_features=14000, out_features=1000, bias=True)\n",
      "  (fc2): Linear(in_features=1000, out_features=9, bias=True)\n",
      ")\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import math\n",
    "\n",
    "# for evaluating the model\n",
    "from sklearn.metrics import accuracy_score\n",
    "from tqdm import tqdm\n",
    "\n",
    "# PyTorch libraries and modules\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.autograd import Variable\n",
    "import torch.nn.functional as F\n",
    "from torch.nn import Linear, ReLU, CrossEntropyLoss, Sequential, Conv2d, MaxPool2d, Module, Softmax, BatchNorm2d, Dropout\n",
    "from torch.optim import Adam, SGD\n",
    "from torch.nn.utils.rnn import pack_padded_sequence, pad_packed_sequence\n",
    "import itertools\n",
    "\n",
    "# empty list to store training losses\n",
    "train_losses = []\n",
    "# empty list to store validation losses\n",
    "val_losses = []\n",
    "    \n",
    "def text_to_vocab(text_arr):\n",
    "    v = {}\n",
    "    all_freq = {} \n",
    "    for text in text_arr:\n",
    "        for i in text: \n",
    "            if i in all_freq: \n",
    "                all_freq[i] += 1\n",
    "            else: \n",
    "                all_freq[i] = 1\n",
    "    v[spl_char] = 0\n",
    "    for i in all_freq:\n",
    "        if(all_freq[i] >= 10):\n",
    "            v[i] = all_freq[i]\n",
    "        else:\n",
    "            v[spl_char] = v[spl_char] + all_freq[i]\n",
    "    return v\n",
    "\n",
    "class Net(Module):   \n",
    "    def __init__(self):\n",
    "        super(Net, self).__init__()\n",
    "        self.layer1 = nn.Sequential(\n",
    "            nn.Conv2d(1, 50, kernel_size=5, stride=1, padding=2),\n",
    "            nn.ReLU(),\n",
    "            nn.MaxPool2d(kernel_size=2, stride=2))\n",
    "        self.layer2 = nn.Sequential(\n",
    "            nn.Conv2d(50, 100, kernel_size=5, stride=1, padding=2),\n",
    "            nn.ReLU(),\n",
    "            nn.MaxPool2d(kernel_size=2, stride=2))\n",
    "        self.drop_out = nn.Dropout()\n",
    "        self.fc1 = nn.Linear(280 * 50, 1000)\n",
    "        self.fc2 = nn.Linear(1000, 9)\n",
    "\n",
    "    def forward(self, x):\n",
    "        out = self.layer1(x)\n",
    "        out = self.layer2(out)\n",
    "        out = out.reshape(out.size(0), -1)\n",
    "        out = self.drop_out(out)\n",
    "        out = self.fc1(out)\n",
    "        out = self.fc2(out)\n",
    "        return out\n",
    "\n",
    "# defining the model\n",
    "model = Net()\n",
    "# defining the optimizer\n",
    "optimizer = Adam(model.parameters(), lr=0.07)\n",
    "# defining the loss function\n",
    "criterion = CrossEntropyLoss()\n",
    "    \n",
    "print(model)\n",
    "\n",
    "def train(x_train, y_train):\n",
    "    model.train()\n",
    "    tr_loss = 0\n",
    "\n",
    "    # clearing the Gradients of the model parameters\n",
    "    optimizer.zero_grad()\n",
    "    \n",
    "    # prediction for training and validation set\n",
    "    output_train = model(x_train)\n",
    "\n",
    "    # computing the training and validation loss\n",
    "    loss_train = criterion(output_train, y_train)\n",
    "    train_losses.append(loss_train)\n",
    "\n",
    "    # computing the updated weights of all the model parameters\n",
    "    loss_train.backward(retain_graph=True)\n",
    "    optimizer.step()\n",
    "    tr_loss = loss_train.item()\n",
    "    return tr_loss\n",
    "    \n",
    "    \n",
    "def index_of(tok):\n",
    "    vocab_list = list(vocab.keys())\n",
    "    if tok in vocab_list:\n",
    "        return vocab_list.index(tok)\n",
    "    else:\n",
    "        return 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "df_train= pd.read_csv('/Users/mukulj/Downloads/Neha Project/Assignment5/train.csv', names=['language', 'document'])\n",
    "df_val= pd.read_csv('/Users/mukulj/Downloads/Neha Project/Assignment5/val.csv', names=['language', 'document'])\n",
    "df_test= pd.read_csv('/Users/mukulj/Downloads/Neha Project/Assignment5/test.csv', names=['language', 'document'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'es': 0,\n",
       " 'en': 1,\n",
       " 'pt': 2,\n",
       " 'fr': 3,\n",
       " 'ca': 4,\n",
       " 'de': 5,\n",
       " 'eu': 6,\n",
       " 'it': 7,\n",
       " 'gl': 8}"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "language_map = {}\n",
    "i = 0\n",
    "for lan in df_train.language.unique():\n",
    "    language_map[lan] = i\n",
    "    i = i+1\n",
    "df_train.language.replace(language_map, inplace=True)\n",
    "df_val.language.replace(language_map, inplace=True)\n",
    "df_test.language.replace(language_map, inplace=True)\n",
    "language_map"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cross_entropy: 5.0599491674909345 \n",
      "perplexity: 33.35772898629834\n"
     ]
    }
   ],
   "source": [
    "spl_char = 'out-of-vocabulary'\n",
    "vocab = {}\n",
    "\n",
    "train_tweets = []\n",
    "for i in range(df_train.shape[0]):\n",
    "    text = df_train.document[i]\n",
    "    train_tweets.append(text)\n",
    "\n",
    "vocab = text_to_vocab(train_tweets)\n",
    "\n",
    "relative_freq = {}\n",
    "s = sum(vocab.values())\n",
    "for x in vocab:\n",
    "    relative_freq[x] = vocab[x]/s\n",
    "    \n",
    "val_tweets = []\n",
    "for i in range(df_val.shape[0]):\n",
    "    text = df_val.document[i]\n",
    "    val_tweets.append(text)\n",
    "    \n",
    "logpx = []\n",
    "for t in val_tweets:\n",
    "    for x in t:\n",
    "        if x in relative_freq:\n",
    "            logpx.append(math.log2(relative_freq[x]))\n",
    "        else:\n",
    "            logpx.append(math.log2(relative_freq[spl_char]))\n",
    "\n",
    "cross_entropy = (-1)*sum(logpx)/len(logpx)\n",
    "perplexity = 2**cross_entropy\n",
    "\n",
    "print('cross_entropy:', cross_entropy, '\\nperplexity:', perplexity)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_tweets = df_train.document.str.slice(0, 280)\n",
    "val_tweets = df_val.document.str.slice(0, 280)\n",
    "train_vectorized_seqs = [[index_of(tok) for tok in seq]for seq in train_tweets]\n",
    "val_vectorized_seqs = [[index_of(tok) for tok in seq]for seq in val_tweets]\n",
    "\n",
    "# get the length of each seq in your batch\n",
    "train_seq_lengths = torch.LongTensor(list(map(len, train_vectorized_seqs)))\n",
    "val_seq_lengths = torch.LongTensor(list(map(len, val_vectorized_seqs)))\n",
    "\n",
    "# dump padding everywhere, and place seqs on the left.\n",
    "# NOTE: you only need a tensor as big as your longest sequence\n",
    "train_seq_tensor = Variable(torch.zeros((len(train_vectorized_seqs), 280))).long()\n",
    "for idx, (seq, seqlen) in enumerate(zip(train_vectorized_seqs, train_seq_lengths)):\n",
    "    train_seq_tensor[idx, :seqlen] = torch.LongTensor(seq)\n",
    "    \n",
    "val_seq_tensor = Variable(torch.zeros((len(val_vectorized_seqs), 280))).long()\n",
    "for idx, (seq, seqlen) in enumerate(zip(val_vectorized_seqs, val_seq_lengths)):\n",
    "    val_seq_tensor[idx, :seqlen] = torch.LongTensor(seq)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "embedding = nn.Embedding(len(vocab), 10)\n",
    "\n",
    "train_x = embedding(train_seq_tensor)\n",
    "val_x = embedding(val_seq_tensor)\n",
    "\n",
    "train_y = torch.tensor(df_train.language.values)\n",
    "val_y = torch.tensor(df_val.language.values)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# getting the training set\n",
    "x_train, x_val = train_x.reshape(76875, 1, 280, 10), val_x.reshape(11128, 1, 280, 10)\n",
    "# getting the validation set\n",
    "y_train, y_val = Variable(train_y), Variable(val_y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "batch 0 2.2527060508728027\n",
      "batch 1 5570.65869140625\n",
      "batch 2 128577.1875\n",
      "batch 3 489.87872314453125\n",
      "batch 4 6.632012367248535\n"
     ]
    }
   ],
   "source": [
    "def batch(iterable, n=100):\n",
    "    l = len(iterable)\n",
    "    for ndx in range(0, l, n):\n",
    "        yield iterable[ndx:min(ndx + n, l)]\n",
    "i = 0\n",
    "# training the model    \n",
    "for x_t, y_t in zip(batch(x_train), batch(y_train)):\n",
    "    print('batch', i, train(x_t, y_t))\n",
    "    i = i+1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
